{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ce582e",
   "metadata": {},
   "source": [
    "# Compress and Merge DataFrames\n",
    "----\n",
    "\n",
    "This notebook aims to clean, compress and merge the datasets from collected from <a href=\"https://www.consumerfinance.gov/data-research/hmda/historic-data/\">Consumer Finance</a>.\n",
    "\n",
    "Each dataset on average is 700MB and this inhibted team collaboration. To counteract this a single dataset was needed which complied with GitHub Enterprise's 100MB limitations.\n",
    "\n",
    "In order to achieve this:\n",
    " - Data reduced to only the Metro Areas in Florida, New York, Texas and California\n",
    " - Removed Income Outliers. (Applicants under the 20<sup>th</sup> percentile and over the 90<sup>th</sup> percentile)\n",
    " - Compressed all datapoints to numeric key:value pairs\n",
    " - Used GZip compression to reduce the csv file from 612.9MB to 99.8MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2bd26a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706f0bb6",
   "metadata": {},
   "source": [
    "## Function Definition Block\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bdf92466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latino_bin(value):\n",
    "    if value == 'Hispanic or Latino': output = 1\n",
    "    else: output = 0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "588effac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loan_approved_bin(value):\n",
    "    accepted = [\"Loan originated\",\n",
    "                \"Application approved but not accepted\",\n",
    "                \"Loan purchased by the institution\"]\n",
    "    if value in accepted:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1200fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compression_dict(df, columns, compress=True):\n",
    "    \"\"\"Creates a dictionary of enumerated key:value pairs in a given Pandas DataFrame\n",
    "    based on a list of columns. This can then be mapped to the dataframe to compress\n",
    "    the datapoints to a numeric value or vice versa.\n",
    "    \n",
    "    - Function defaults to a compression format where the keys are the original data\n",
    "      and the values are numbers.\n",
    "    - if compress is False it returns a dictionary where the keys are numbers and the\n",
    "      values are the original datapoints.\"\"\"\n",
    "    df_dict = {}\n",
    "    for col in columns:\n",
    "        column_dict = {}\n",
    "        if compress:\n",
    "            for value, key in enumerate(df[col].value_counts().index):\n",
    "                column_dict[key] = value\n",
    "        else:\n",
    "            for value, key in enumerate(df[col].value_counts().index):\n",
    "                column_dict[value] = key\n",
    "        df_dict[col] = column_dict\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7128a49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_dataframe(df, dictionary):\n",
    "    \"\"\"Simple function to map the compression/decompression dictionary to a DF.\"\"\"\n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    for col in dictionary.keys():\n",
    "        df[col] = df[col].map(dictionary[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c589274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_merge_dataframes(directory):\n",
    "    \"\"\"Reads all ***FILES*** in a directory and attempts to read them as CSV.\n",
    "    ***Will Break if Non-CSV files are in directory***\n",
    "    Also culls data to:\n",
    "     - Income over 20th percentile, under 90th percentile\n",
    "     - cities based on MSAMD\n",
    "     - Adds Latino Bin\n",
    "     - Adds Approve Bin\"\"\"\n",
    "    dataframes = []\n",
    "    files = os.listdir(directory)\n",
    "    for i, file in enumerate(files):\n",
    "        file_path = f\"./cfpb_data/{file}\"\n",
    "        print(f\"Pulling file {i+1}/{len(files)}\")\n",
    "    \n",
    "        #read csv, add state column, make a list of df's\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        \n",
    "        # Limit incomes to >20% and <90%\n",
    "        prcntl = df['applicant_income_000s'].quantile([0.20,0.90]).values\n",
    "        df = df[(df['applicant_income_000s'] > prcntl[0]) &\n",
    "                (df['applicant_income_000s'] < prcntl[1])]\n",
    "        \n",
    "        #limit cities\n",
    "        cities = [36740.0, 27260.0, 33124.0, 45300.0, #Florida\n",
    "                  31084.0, 36084.0, 41740.0, #California\n",
    "                  19124.0, 12420.0, 26420.0, #Texas\n",
    "                  35614.0, 35004.0] #New York\n",
    "        df = df[df[\"msamd\"].isin(cities)]\n",
    "        \n",
    "        #add approve bin\n",
    "        df[\"approve_bin\"] = df[\"action_taken_name\"].map(lambda x: loan_approved_bin(x))\n",
    "        \n",
    "        #Make a column of Latino, not Latinto\n",
    "        df[\"latino\"] = df['applicant_ethnicity_name'].map(lambda x: latino_bin(x))\n",
    "        \n",
    "        dataframes.append(df)\n",
    "    print(\"Merging DataFrames\")\n",
    "    return pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ce9633",
   "metadata": {},
   "source": [
    "# Main Block\n",
    "----\n",
    " 1. Read and Merge DataFrames\n",
    " 2. Reduce Features\n",
    " 3. Create a Compression and Decompression Dictionary\n",
    " 4. Output Compression and Decompression Dictionary\n",
    " 5. Compress DataFrame\n",
    " 6. Output Compressed DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c0409d",
   "metadata": {},
   "source": [
    "### Read and Merge DataFrames\n",
    "----\n",
    "Initial size of all files in the folder were 13.3GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "669a1c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling file 1/16\n",
      "Pulling file 2/16\n",
      "Pulling file 3/16\n",
      "Pulling file 4/16\n",
      "Pulling file 5/16\n",
      "Pulling file 6/16\n",
      "Pulling file 7/16\n",
      "Pulling file 8/16\n",
      "Pulling file 9/16\n",
      "Pulling file 10/16\n",
      "Pulling file 11/16\n",
      "Pulling file 12/16\n",
      "Pulling file 13/16\n",
      "Pulling file 14/16\n",
      "Pulling file 15/16\n",
      "Pulling file 16/16\n",
      "Merging DataFrames\n"
     ]
    }
   ],
   "source": [
    "merged_df = read_and_merge_dataframes(\"./cfpb_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3b2047",
   "metadata": {},
   "source": [
    "### Reduce Features\n",
    "----\n",
    "In order to reduce the size some of the less relavant features should be cut. Based on Vadim's EDA the DataFrames can be reduced to the list below. Result is a reduction from 80 features to 52 features. 28 features across 4,715,850 rows should reduce the size of the file substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a9d9406f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4715850, 80)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5786375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_list=['as_of_year', 'respondent_id', 'agency_name', 'agency_abbr',\n",
    "       'agency_code', 'loan_type_name', 'loan_type', 'loan_purpose_name',\n",
    "       'loan_purpose', 'owner_occupancy_name', 'owner_occupancy', 'loan_amount_000s',\n",
    "       'preapproval_name', 'preapproval', 'action_taken_name', 'action_taken',\n",
    "       'msamd_name', 'msamd', 'state_name', 'county_name', 'census_tract_number',\n",
    "       'applicant_ethnicity_name', 'applicant_ethnicity', 'co_applicant_ethnicity_name',\n",
    "       'co_applicant_ethnicity', 'applicant_race_name_1', 'applicant_race_1',\n",
    "       'co_applicant_race_name_1', 'co_applicant_race_1', 'applicant_sex_name',\n",
    "       'co_applicant_sex_name', 'applicant_income_000s',\n",
    "       'purchaser_type_name', 'purchaser_type', 'denial_reason_name_1',\n",
    "       'denial_reason_1', 'denial_reason_name_2', 'denial_reason_2',\n",
    "       'denial_reason_name_3', 'denial_reason_3', 'hoepa_status_name', 'hoepa_status',\n",
    "       'lien_status_name', 'lien_status', 'population', 'minority_population',\n",
    "       'hud_median_family_income', 'tract_to_msamd_income', 'number_of_owner_occupied_units',\n",
    "       'number_of_1_to_4_family_units', \"latino\", \"approve_bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a4773efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df=merged_df[indices_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "631b36c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4715850, 52)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc37c3",
   "metadata": {},
   "source": [
    "## Create a Compression Dictionary\n",
    "----\n",
    "\n",
    "The datasets are massive. Simply reducing the features and keeping it in a zip is enough to get it to a size that we can put on GitHub. In order to reduce the file size further all values in the final dataset are going to be stored as numerical and translated back as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9eaa37",
   "metadata": {},
   "source": [
    "### Create Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0ea82787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated. Even numerical columns needed to be compressed\n",
    "categorical_features = merged_df.select_dtypes(exclude =\"number\").columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c3e8bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_dictionary = create_compression_dict(merged_df, merged_df.columns)\n",
    "decompression_dictionary = create_compression_dict(merged_df, merged_df.columns, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6359272",
   "metadata": {},
   "source": [
    "### Output Compression and Decompression Dictionary\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1460f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"json_files/compression_dictionary.json\", \"w\") as json_file:\n",
    "    json.dump(compression_dictionary, json_file)\n",
    "\n",
    "with open(\"json_files/decompression_dictionary.json\", \"w\") as json_file:\n",
    "    json.dump(decompression_dictionary, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d4eaad",
   "metadata": {},
   "source": [
    "## Compress the file\n",
    "----\n",
    "Using GZip formatting reduced final CSV file from 612,926 KB to 99,841 KB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d5428073",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = compress_dataframe(merged_df, compression_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f53f44f",
   "metadata": {},
   "source": [
    "## Output Compressed DataFrame\n",
    "----\n",
    "End result is a 13,321% decrease in size from the original 13.3GB to the finalized 99.8MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "250b01ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"./merged_df/merged_df.csv.gz\", index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e570d-d184-457d-8234-59cd28c30709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
